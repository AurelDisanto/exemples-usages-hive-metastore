{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eefef6-c2af-4698-b903-418330a5676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls -t   \"s3a://projet-spark-lab/diffusion/tweets/input\" | grep \"tweets\"| head -n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d908957-02f4-4f0a-a860-961ea7cd1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls -r -t \"s3a://projet-spark-lab/diffusion/tweets/input\" | head -n2 |awk '{print $8}' | xargs -I{} hadoop fs -cat {} | head -n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dec5ff-0c6a-4107-8353-41e566d41f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "import pickle\n",
    "from pyspark.sql.functions import explode,split\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import window, col,from_utc_timestamp,to_timestamp,explode, split\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5571b-455a-42f3-ad43-82447624ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "#url par défaut d'une api kubernetes accédé depuis l'intérieur du cluster (ici le notebook tourne lui même dans kubernetes)\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc:443\")\n",
    "\n",
    "#image des executors spark: pour des raisons de simplicité on réutilise l'image du notebook\n",
    "conf.set(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n",
    "\n",
    "# Nom du compte de service pour contacter l'api kubernetes : attention le package du datalab crée lui même cette variable d'enviromment.\n",
    "# Dans un pod du cluster kubernetes il faut lire le fichier /var/run/secrets/kubernetes.io/serviceaccount/token\n",
    "# Néanmoins ce paramètre est inutile car le contexte kubernetes local de ce notebook est préconfiguré\n",
    "# conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \n",
    "\n",
    "# Nom du namespace kubernetes\n",
    "conf.set(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n",
    "\n",
    "# Nombre d'executeur spark, il se lancera autant de pods kubernetes que le nombre indiqué.\n",
    "conf.set(\"spark.executor.instances\", \"5\")\n",
    "\n",
    "# Mémoire alloué à la JVM\n",
    "# Attention par défaut le pod kubernetes aura une limite supérieur qui dépend d'autres paramètres.\n",
    "# On manipulera plus bas pour vérifier la limite de mémoire totale d'un executeur\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "\n",
    "# Paramètres d'enregistrement des logs spark d'application\n",
    "# Attention ce paramètres nécessitent la création d'un dossier spark-history. Spark ne le fait pas lui même pour des raisons obscurs\n",
    "# import s3fs\n",
    "# endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "# fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "# fs.touch('s3://tm8enk/spark-history/.keep')\n",
    "# sparkconf.set(\"spark.eventLog.enabled\",\"true\")\n",
    "# sparkconf.set(\"spark.eventLog.dir\",\"s3a://tm8enk/spark-history\")\n",
    "#ici pour gérer le dateTimeFormatter dépendant de la verion de java...\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "#conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"streaming\").config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb711b2-41ba-424a-91df-bf02bfce883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/auredisanto/data-exemples-avantages-spark/spark-streaming/schema.p ~/work/schema.p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f901b-e152-4fa4-b5e6-b54c2f682537",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pickle.load(open(\"/home/onyxia/work/schema.p\", \"rb\" ))\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508e901-76ab-494f-964b-31748bb104f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"json\")  \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"latestFirst\",\"true\") \\\n",
    "    .load(\"s3a://projet-spark-lab/diffusion/tweets/input\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b0638-a9ea-4dd5-9d8f-441ffd19e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21d311-f327-4adb-9d6b-cc74c4063868",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tab = df.withColumn('word', explode(split(col('text'), ' '))) \\\n",
    "    .filter(col('word').contains('#')) \\\n",
    "    .groupBy('word') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c634b-f143-4f92-b810-b7a86a097093",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tab.writeStream. \\\n",
    "    outputMode(\"complete\"). \\\n",
    "    format(\"memory\"). \\\n",
    "    queryName(\"tweetquery_group_hashtag\"). \\\n",
    "    trigger(processingTime='10 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd4d66-67a6-47c6-bde2-a9bb841e18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "    print(\"streaming\", stream.name, \"avec l'id\", stream.id, \"en cours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6df26-fdfe-4772-a610-5974d5844bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "    print(\"streaming\", stream.name, \"avec l'id\", stream.id, \"en cours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca0d74-15c0-466a-95f9-83c5c79900a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from tweetquery_group_hashtag order by count desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa5add-a193-4867-a118-aa194e62aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from tweetquery_group_hashtag order by count desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc2335-92ed-49ee-9f0e-009db0d596aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tab_24=df \\\n",
    "    .withColumn(\"timestamp\",to_timestamp('created_at', 'EEE MMM d HH:mm:ss Z yyyy')) \\\n",
    "    .withColumn(\"word\",explode(split(\"text\",' '))) \\\n",
    "    .filter(col(\"word\").contains('#')) \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"3 hours\",\"5 minutes\"),\n",
    "        \"word\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88748e1e-7eff-4c4a-bc3c-45413f9853fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tab_24.writeStream.outputMode(\"append\").trigger(processingTime='1 minute').format(\"memory\").queryName(\"data\").start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a76ce-42d7-4867-b8d3-932a456a65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from data').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9fce88-837c-4001-b5a6-5863c17dd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    clear_output(wait=True)\n",
    "    print(\"A\", datetime.now(), \"le top 20 des hastags sur les tweets mentionnait l'insee dans les 3 dernières heures est :\")\n",
    "    display(spark.sql(\"select * from data where window.start > current_timestamp()-INTERVAL 200 minutes order by word desc\" ).show())\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73ea00-af22-4204-a0bc-5ca374738d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
